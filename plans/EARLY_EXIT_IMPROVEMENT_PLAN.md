# Early Exit SSDLite Improvement Plan

## Executive Summary

Current early exit rate: **7.59%** (11 out of 145 validation images)

**Root Cause:** Early exit point at layer 8 (80 channels) is too shallow for object detection task. The early branch cannot achieve competitive accuracy with the full model (672 channels), causing the model to avoid early exits.

**Recommendation:** Multi-level cascade architecture with strengthened branches and multi-class training.

---

## Current Problem Analysis

### What the 7.59% Represents

The 7.59% is the percentage of **entire validation images** that exited early, NOT individual chair detections. The model makes an early exit decision at the **image level**, averaging confidence across all anchor predictions.

### Key Issues

1. **Extremely low early exit rate**: Only 7.59% of samples exit early (11 out of 145)
   - 92.4% of samples still go through the full network

2. **Large performance gap**:
   - Early branch loss: **0.159**
   - Full network loss: **2.27e-08** (essentially zero)
   - This huge gap indicates the early branch is significantly underperforming

3. **No computational savings**: With only 7.6% early exits, minimal speed improvement

4. **Training dynamics**:
   - Early exit rate peaked at 12.4% (epoch 14) then dropped to 7.6%
   - The model learned to avoid the early exit because the full network is far more accurate

5. **Confidence plateau**:
   - Target threshold: 0.85
   - Achieved confidence: ~0.73-0.77
   - **The early branch cannot reach the threshold consistently**

### Architectural Limitations

**Current MobileNetV3 Structure:**
- **Layers 0-7** (â†’80 channels): Early features (edges, textures, basic patterns)
- **Layers 8-11** (â†’112 channels): Mid features (parts, object patterns)
- **Layers 12-15** (â†’672 channels): High features (semantic objects, context)

**Current Early Exit:**
- Layer 8: 80 channels - captures low-level features only
- Full model: 672 channels - 8.4x more feature capacity
- Object detection requires high-level semantic features from deeper layers

---

## Proposed Improvements Analysis

### 1. Multi-Level Early Exits (Layer 8 + Layers 12-14)

**Status:** âœ… **EXCELLENT - Core improvement**

**Rationale:**
- Layer 8 (80ch): Can handle simple backgrounds, obvious negatives
- Layer 12-14 (160ch): Can handle moderate complexity scenes
- Full model (672ch): Handles complex multi-object scenes

**Benefits:**
- Addresses core architectural limitation directly
- Minimal parameter overhead
- Significant potential speedup
- Natural complexity stratification

**Trade-offs:**
- Adds complexity (3 training objectives vs 1)
- Need to tune 2 thresholds instead of 1
- More complex training dynamics

**Expected Impact:**
- Could achieve **60-70% total early exits** vs current 7.6%
- Estimated speedup: **1.8-2.5x**

---

### 2. Strengthen Early Branches

**Status:** âœ… **GOOD - Essential for success**

**Current Branch Architecture:**
- Simple 3-layer depthwise separable convolutions
- Intermediate channels: 80â†’40 (layer 8 branch)

**Proposed Enhancements:**

#### A. Attention Mechanisms
- **SE (Squeeze-and-Excitation) blocks** - channel attention
- **CBAM (Convolutional Block Attention Module)** - channel + spatial
- Cost: 10-20% parameter increase
- Benefit: Significant improvement in feature quality

#### B. Larger Intermediate Channels
- Layer 8 branch: 80â†’128 (vs current 80â†’40)
- Layer 12 branch: 160â†’256 (vs none)
- Better feature extraction capacity

#### C. Feature Pyramid Integration
- Multi-scale awareness for better object detection
- Helps with objects of varying sizes

#### D. Skip Connections
- Better gradient flow
- Helps training stability

**Warning:** Don't make branches too complex - defeats "early" purpose

**Recommendation:**
- Add SE attention blocks (cheap, effective)
- Increase intermediate channels moderately
- Keep branch depth shallow (3-4 layers max)

---

### 3. Lower Confidence Threshold (0.6-0.7 vs 0.85)

**Status:** âš ï¸ **Only viable AFTER architecture fixes**

**Critical Insight:**
- Current threshold: 0.85
- Achieved confidence: ~0.73-0.77 plateau
- **The model literally cannot reach 0.85 consistently**

**BUT - The Catch:**
- Early loss: 0.159 (poor predictions)
- Full loss: 2.27e-08 (essentially perfect)
- Lowering threshold NOW = accepting terrible predictions

**Strategy:**
1. Fix architecture first (Proposals 1 & 2)
2. Measure new confidence distributions per exit level
3. Tune thresholds on validation set for speed/accuracy tradeoff
4. Use different thresholds per exit level:
   - Layer 8 exit: ~0.60 (conservative, backgrounds only)
   - Layer 12 exit: ~0.75 (moderate complexity)

**Expected Thresholds After Fixes:**
```
Exit Level    Threshold    Expected Exit Rate    Use Case
Layer 8       0.60         25-35%               Empty backgrounds, obvious negatives
Layer 12      0.75         40-50%               Simple scenes, single objects
Full Model    N/A          15-25%               Complex multi-object scenes
```

---

### 4. Region-Level vs Image-Level Exits

**Status:** ğŸ”¶ **AMAZING potential, hard to implement efficiently**

**Why It's Perfect for Object Detection:**

Most images have:
- **90% empty background** (easy to detect)
- **10% object regions** (hard to detect)

**Example Scenario:**
```
Image: person standing in empty room
- Background anchors (90%): could exit at layer 8
- Person anchors (10%): need full model

Current approach: processes EVERYTHING through full model
Region-level: saves 90% of computation
```

**The CNN Problem:**

CNNs compute features for the ENTIRE spatial map at once. You can't selectively compute only some regions - it's all or nothing per layer.

**Potential Solutions:**

#### A. Coarse Spatial Grid
- Divide feature map into 4-9 quadrants
- Exit decision per quadrant
- Pros: Simple to implement
- Cons: Still coarse, wasted computation

#### B. Post-hoc Filtering
```
1. Run early branch on full image
2. Identify high-confidence regions
3. Only run full model on low-confidence regions
4. Combine predictions
```
- Pros: Works with existing CNNs
- Cons: Memory overhead, complex merging

#### C. Architecture Change to Vision Transformer
- Can skip individual tokens/patches
- True region-level processing
- Pros: Perfect for this use case
- Cons: Major architecture change, different training

**Practical Compromise (Recommended):**
- Keep image-level for now
- Use **per-anchor confidence weighting** in loss function
- This teaches model to be confident on backgrounds
- Enables future region-level implementation

---

### 5. Cascade Architecture (Progressive Refinement)

**Status:** âœ… **EXCELLENT - Should be core paradigm**

**Paradigm Shift:**

**Current (Binary Bypass):**
```
Early branch â†’ [confident?] â†’ output
              â†˜ [not confident?] â†’ full model â†’ output
              (early predictions discarded)
```

**Cascade (Progressive Refinement):**
```
Early â†’ Mid â†’ Full
  â†“      â†“      â†“
  â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â†’ Progressive refinement
                   (each stage builds on previous)
```

**Benefits:**

1. **No wasted computation** - early predictions always contribute
2. **Easier learning** - later stages refine, not re-learn
3. **Better training signal** - all branches get direct supervision
4. **Aligned with detection literature** (Cascade R-CNN, Feature Pyramid Networks)

**Implementation with Multi-Level Exits:**

```
Layer 8 Exit:  Coarse proposals (filter obvious negatives)
                â†“ (pass proposals as context)
Layer 12 Exit: Refine proposals (handle moderate complexity)
                â†“ (pass refined proposals)
Full Model:    Final refinement (complex scenes only)
```

**Exit Strategy:**
```python
# Pseudo-code
proposals_1 = exit_branch_1(features_layer8)
if confidence(proposals_1) >= threshold_1:
    return proposals_1  # 25-35% of cases

proposals_2 = exit_branch_2(features_layer12, context=proposals_1)
if confidence(proposals_2) >= threshold_2:
    return proposals_2  # 40-50% of cases

final = full_branch(features_full, context=proposals_2)
return final  # 15-25% of cases
```

**Training Strategy:**
- All branches trained simultaneously
- Knowledge distillation from deeper to shallower
- Progressive supervision signals

---

### 6. Multi-Class Training (1000 images per class)

**Status:** âœ… **GOOD for realism, but do AFTER architecture fixes**

**Suggested COCO Classes (varying difficulty):**

| Category | Classes | Why Include |
|----------|---------|-------------|
| Large/Easy | person, car, chair, couch, bed | High exit potential |
| Medium | bicycle, motorcycle, laptop, tv, bottle | Moderate complexity |
| Small/Hard | cell phone, book, cup, wine glass, fork | Challenging cases |
| Deformable | dog, cat, bird | Non-rigid shapes |
| Complex | traffic light, potted plant | Multi-part objects |

**Benefits:**
- More diverse visual features
- Better generalization
- Realistic evaluation
- Prevents overfitting to single class
- Tests exit strategy across difficulty levels

**Drawbacks:**
- Harder task â†’ likely lower exit rates initially
- Longer training time (~10-20x data)
- Class imbalance issues in COCO
- More complex debugging

**Why Do This AFTER Architecture Fixes:**
- Multi-class with current broken architecture = even worse results
- Fix architecture on simple task first
- Then scale to realistic complexity

**Data Strategy:**
- 1000 images per class (10-15 classes = 10-15K images)
- Balanced sampling during training
- Stratified validation split

---

## Synergies & Conflicts

### Powerful Combinations âœ…

1. **Multi-level + Cascade**
   - Natural progression through complexity levels
   - Each level builds on previous
   - Aligned training objectives

2. **Multi-level + Stronger branches**
   - Each level appropriately complex for its task
   - Layer 8: lightweight + attention
   - Layer 12: moderate + attention
   - Balanced complexity budget

3. **Cascade + Attention**
   - Attention focuses on refining previous predictions
   - Better signal propagation
   - Efficient feature reuse

### Conflicts âŒ

1. **Region-level + Current CNN**
   - CNNs compute entire spatial maps
   - Cannot selectively skip regions efficiently
   - Requires major architecture change

2. **Multi-class + Current architecture**
   - Harder task with already failing architecture
   - Will make poor results even worse
   - Do architecture fixes first

---

## Recommended Implementation Plan

### Phase 1: Core Architecture Redesign âš¡ (Do First)

**Goal:** Fix fundamental architectural limitations

**Changes:**

1. âœ… Add second early exit at layer 12-14 (160 channels)
2. âœ… Implement cascade paradigm (progressive refinement)
3. âœ… Add attention modules to both early branches (SE blocks)
4. âœ… Increase early branch capacity:
   - Exit 1: 80â†’128 intermediate channels
   - Exit 2: 160â†’256 intermediate channels
5. âœ… Keep image-level decisions (simpler implementation)
6. âœ… Keep chair-only dataset (isolate architecture effects)

**Expected Architecture:**

```
Input (320Ã—320Ã—3)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layers 0-7: MobileNetV3 blocks â”‚
â”‚ Output: 20Ã—20Ã—80                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Exit Branch 1 (with SE attention)â”‚
â”‚ 80â†’128â†’(cls, reg)               â”‚
â”‚ Proposals_1                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
    Decision: avg_conf >= 0.60?
    â”œâ”€ YES â†’ OUTPUT (25-35% samples)
    â””â”€ NO â†’ Continue
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layers 8-12: MobileNetV3 blocksâ”‚
â”‚ Output: 10Ã—10Ã—160               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ (+ Proposals_1 as context)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Exit Branch 2 (with SE attention)â”‚
â”‚ 160â†’256â†’(cls, reg)              â”‚
â”‚ Refined_Proposals_2              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
    Decision: avg_conf >= 0.75?
    â”œâ”€ YES â†’ OUTPUT (40-50% samples)
    â””â”€ NO â†’ Continue
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layers 13-end: MobileNetV3     â”‚
â”‚ Output: 10Ã—10Ã—672               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ (+ Refined_Proposals_2 as context)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Full Branch                      â”‚
â”‚ 672â†’(cls, reg)                  â”‚
â”‚ Final_Detections                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
    OUTPUT (15-25% samples)
```

**Training Strategy:**

```python
# Loss weighting
total_loss = (
    0.3 * exit1_loss +        # Early branch
    0.4 * exit2_loss +        # Mid branch
    0.3 * full_loss +         # Full branch
    0.05 * diversity_loss +   # Confidence distribution
    0.05 * cascade_loss       # Inter-branch consistency
)

# Cascade loss: ensure smooth refinement
cascade_loss = (
    kl_div(exit2, exit1.detach()) +  # Exit2 refines Exit1
    kl_div(full, exit2.detach())     # Full refines Exit2
)
```

**Expected Results:**
- Exit 1 (layer 8): 25-35% samples (empty backgrounds)
- Exit 2 (layer 12): 40-50% samples (simple scenes)
- Full model: 15-25% samples (complex scenes)
- Weighted accuracy drop: <5%
- Average speedup: **1.8-2.5x**
- Early exit rate: **60-75% combined**

---

### Phase 2: Threshold Tuning ğŸ¯ (After Phase 1 works)

**Goal:** Optimize speed/accuracy tradeoff

**Steps:**

1. **Train with Phase 1 architecture**
   - 20 epochs on chair dataset
   - Monitor confidence distributions per exit level

2. **Measure confidence distributions**
   ```python
   # For each exit level on validation set
   - Mean confidence
   - Std confidence
   - Confidence histogram
   - Confidence vs accuracy correlation
   ```

3. **Grid search thresholds**
   ```python
   threshold_1 = [0.50, 0.55, 0.60, 0.65]
   threshold_2 = [0.65, 0.70, 0.75, 0.80]

   # Evaluate on validation set
   for t1 in threshold_1:
       for t2 in threshold_2:
           measure_metrics(t1, t2)
   ```

4. **Per-branch temperature scaling**
   - Calibrate confidence scores
   - Learnable temperature parameters
   - Better calibration â†’ better threshold decisions

5. **Select operating point**
   - Plot speed vs accuracy curve
   - Choose based on application requirements
   - E.g., 2x speedup with <3% accuracy drop

**Metrics to Track:**
- Exit rate per level
- Accuracy per exit level
- Average inference time
- Speedup vs full model
- FLOPs reduction

---

### Phase 3: Multi-Class Scaling ğŸ“ˆ (After Phase 2 validates)

**Goal:** Validate on realistic multi-class detection

**Steps:**

1. **Select 10-15 COCO classes**
   - Balanced across difficulty levels
   - 1000 images per class
   - Total: 10-15K training images

2. **Update model head**
   - Change num_classes from 2 to 11-16 (background + classes)
   - Adjust anchor matching strategy
   - Handle class imbalance

3. **Retrain from scratch**
   - Use Phase 1 architecture
   - Use Phase 2 optimal thresholds as starting point
   - 30-40 epochs (more data = longer training)

4. **Evaluate generalization**
   - Per-class exit rates
   - Per-class accuracy
   - Identify which classes benefit most from early exit
   - Adjust thresholds per class if needed

5. **Analysis**
   - Which classes exit early? (likely: backgrounds, large objects)
   - Which need full model? (likely: small objects, occlusion)
   - Use insights to further optimize architecture

**Expected Insights:**
```
Easy classes (high exit rate):
- Background, sky, walls
- Large objects: person, car, bed

Hard classes (low exit rate):
- Small objects: fork, cell phone
- Occluded/partial objects
- Multiple overlapping objects
```

---

### Phase 4: Advanced (If Needed) ğŸš€

**Goal:** Explore cutting-edge approaches

#### A. Region-Level Exits
- Requires significant rearchitecture
- Vision Transformer backbone enables spatial selection
- Can skip individual patches/tokens
- Massive potential speedup (5-10x)

#### B. Dynamic Routing
- Learned routing network
- Predicts optimal exit level per sample
- End-to-end differentiable
- Better than fixed thresholds

#### C. Neural Architecture Search (NAS)
- Automatically find optimal exit points
- Optimize branch architectures
- Trade-off search space

#### D. Quantization + Pruning
- Combine with early exit
- Ultra-efficient deployment
- Edge device deployment

---

## Implementation Priority

### Must Have (Phase 1)
1. âœ… Multi-level exits (layer 8 + layer 12)
2. âœ… Cascade architecture
3. âœ… SE attention blocks
4. âœ… Larger intermediate channels

### Should Have (Phase 2)
1. âœ… Threshold tuning
2. âœ… Temperature scaling
3. âœ… Comprehensive metrics

### Nice to Have (Phase 3)
1. âœ… Multi-class training
2. âœ… Per-class analysis

### Research Level (Phase 4)
1. ğŸ”¬ Region-level exits
2. ğŸ”¬ Vision Transformer
3. ğŸ”¬ Dynamic routing

---

## Success Metrics

### Current Baseline
- Early exit rate: 7.59%
- Early loss: 0.159
- Full loss: 2.27e-08
- Speedup: ~1.0x (negligible)

### Phase 1 Targets
- Combined early exit rate: >60%
- Exit 1 loss: <0.05
- Exit 2 loss: <0.02
- Full loss: <1e-05
- Average speedup: 1.8-2.5x
- Accuracy drop: <5%

### Phase 2 Targets
- Optimized exit rates: 70-80%
- Average speedup: 2.0-3.0x
- Accuracy drop: <3%

### Phase 3 Targets
- Multi-class validation
- Consistent performance across classes
- Generalization validation

---

## Risk Analysis

### High Risk âš ï¸
1. **Multi-level training stability**
   - Mitigation: Careful loss weighting, gradual threshold tuning

2. **Cascade coordination**
   - Mitigation: Strong supervision signals, detached gradients where needed

### Medium Risk âš¡
1. **Overfitting to chairs**
   - Mitigation: Phase 3 multi-class validation

2. **Threshold sensitivity**
   - Mitigation: Temperature scaling, extensive validation

### Low Risk âœ…
1. **Implementation complexity**
   - Well-understood components
   - Incremental development

---

## Timeline Estimate

| Phase | Tasks | Estimated Time |
|-------|-------|----------------|
| Phase 1 | Architecture implementation | 3-5 days |
| Phase 1 | Training & debugging | 2-3 days |
| Phase 2 | Threshold tuning | 1-2 days |
| Phase 3 | Multi-class setup & training | 3-4 days |
| Phase 3 | Analysis & iteration | 2-3 days |
| **Total** | **Complete implementation** | **11-17 days** |

---

## Conclusion

The current 7.59% early exit rate is due to **fundamental architectural limitations**, not data scarcity. The early exit point at layer 8 simply cannot compete with the full 672-channel model for object detection.

**Key Insights:**
1. Single early exit at layer 8 is too shallow for object detection
2. Multi-level cascade architecture is the solution
3. Attention mechanisms and larger capacity are essential
4. Multi-class training validates but doesn't fix core issues
5. Region-level exits are promising but require major changes

**Recommended Path:**
Start with **Phase 1** (multi-level cascade with attention). This has the highest probability of success with reasonable implementation effort. Expected improvement: **7.6% â†’ 60-75% exit rate** with **2-3x speedup**.

---

## References

- Cascade R-CNN: High Quality Object Detection
- Feature Pyramid Networks for Object Detection
- Squeeze-and-Excitation Networks (SE)
- BranchyNet: Fast Inference via Early Exiting
- Multi-Scale Dense Networks for Resource Efficient Image Classification
